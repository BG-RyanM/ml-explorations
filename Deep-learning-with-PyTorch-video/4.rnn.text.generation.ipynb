{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with PyTorch\n",
    "\n",
    "\n",
    "Author: [Anand Saha](http://teleported.in/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Sequence Models - RNN for text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a language model based on Shakespeare's writings, and will then generate new text similar to Shakespear's "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simplified version of https://github.com/pytorch/examples/tree/master/word_language_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.cuda as cuda\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helper class to read in the texts, convert the words to integer indexes and provide lookup tables to convert any word to its index and vice versa.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "class Dictionary(object):\n",
    "    \"\"\"Maps word (e.g. `cat`) to an index (e.g. 5) and vice-versa.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "\n",
    "    def add_word(self, word):\n",
    "        \"\"\"Adds word, if not already in dictionary, and returns its index.\"\"\"\n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word] = len(self.idx2word) - 1\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)\n",
    "\n",
    "\n",
    "class Corpus(object):\n",
    "    \"\"\"\n",
    "    Used to load text files (training and validation), generate dictionary from contents,\n",
    "    and tokenize each file (words --> integers).\n",
    "    \"\"\"\n",
    "    def __init__(self, path):\n",
    "        self.dictionary = Dictionary()\n",
    "        \n",
    "        # This is very English language specific.\n",
    "        # We will ingest only these characters:\n",
    "        self.whitelist = [chr(i) for i in range(32, 127)]\n",
    "        \n",
    "        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n",
    "        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n",
    "\n",
    "    def tokenize(self, path):\n",
    "        \"\"\"\n",
    "        Tokenizes a text file. Converts each word in source text to integer ID. Returns\n",
    "        tensor containing sequence of these IDs.\n",
    "        \"\"\"\n",
    "        assert os.path.exists(path)\n",
    "        # Add words to the dictionary\n",
    "        with open(path, 'r',  encoding=\"utf8\") as f:\n",
    "            tokens = 0  # counts number of tokens, i.e. number of words in source text\n",
    "            for line in f:\n",
    "                line = ''.join([c for c in line if c in self.whitelist])\n",
    "                words = line.split() + ['<eos>']\n",
    "                tokens += len(words)\n",
    "                for word in words:\n",
    "                    self.dictionary.add_word(word)\n",
    "\n",
    "        # Tokenize file content\n",
    "        with open(path, 'r',  encoding=\"utf8\") as f:\n",
    "            # One-dimensional tensor of length 'tokens'\n",
    "            ids = torch.LongTensor(tokens)\n",
    "            token = 0\n",
    "            for line in f:\n",
    "                line = ''.join([c for c in line if c in self.whitelist])\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    ids[token] = self.dictionary.word2idx[word]\n",
    "                    token += 1\n",
    "\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.txt  valid.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls data/shakespear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus('./data/shakespear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# Verify that dictionary works\n",
    "print(corpus.dictionary.idx2word[10])\n",
    "print(corpus.dictionary.word2idx['That'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1039900])\n",
      "torch.Size([63420])\n"
     ]
    }
   ],
   "source": [
    "# About a million words in training set, 63k in validation set\n",
    "print(corpus.train.size())\n",
    "print(corpus.valid.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'else'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is 112th word in training set?\n",
    "id = corpus.train[112]\n",
    "corpus.dictionary.idx2word[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74010\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(corpus.dictionary)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Added by Ryan to deal with out-of-memory errors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The RNN model (using GRU cells)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout=0.5):\n",
    "        \"\"\"\n",
    "        :param vocab_size: number of words in dictionary\n",
    "        :embed_size: see word embedding technique, vectors used to represent words are of this\n",
    "            length\n",
    "        :hidden_size: ???\n",
    "        \"\"\"\n",
    "        \n",
    "        super(RNNModel, self).__init__()\n",
    "        \n",
    "        # Will map an ID to a vector of dims embed_size. Is trained via backpropagation like\n",
    "        # everything else.\n",
    "        self.encoder = nn.Embedding(vocab_size, embed_size)\n",
    "        self.drop1 = nn.Dropout(dropout)\n",
    "        self.drop2 = nn.Dropout(dropout)\n",
    "        # GRU params\n",
    "        #   input: number of input features\n",
    "        #   hidden_size: number of features in hidden layer\n",
    "        #   num_layers: number of of recurrent layers, i.e. number of stacked GRUs\n",
    "        #   dropout = dropout probability\n",
    "        self.rnn = nn.GRU(embed_size, hidden_size, num_layers, dropout=dropout)\n",
    "        self.decoder = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.forward_called = False\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.fill_(0)\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, input_data, hidden):\n",
    "        \"\"\"\n",
    "        Standard function to apply NN to input.\n",
    "        \n",
    "        :param input: input_data features. A 2d tensor of batch data. Go down the rows\n",
    "            to advance through sequences, across the columns to switch sequence.\n",
    "        :param hidden: hidden features from previous time step\n",
    "        \"\"\"\n",
    "        emb = self.drop1(self.encoder(input_data))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        if not self.forward_called:\n",
    "            self.forward_called = True\n",
    "            print(\"first forward() call, input shape is\", input_data.shape)\n",
    "            print(\"hidden shape is\", hidden.shape)\n",
    "            print(\"emb shape is\", emb.shape)\n",
    "            print(\"output shape is\", output.shape)\n",
    "        output = self.drop2(output)\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        return Variable(weight.new(self.num_layers, batch_size, self.hidden_size).zero_())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data, batch_size):\n",
    "    \"\"\"\n",
    "    Split data into batches, each of length batch_size.\n",
    "    If batch size is n, then the data will be divided into n subsequences,\n",
    "    with a subsequence being a contiguous series of words from the\n",
    "    original text.\n",
    "    \n",
    "    Example:\n",
    "    subsequence 0: \"once upon a time...\"\n",
    "    subsequence 1: \"a good king and...\"\n",
    "    \n",
    "    Batch 0 contains first token from each subsequence, batch 1 contains\n",
    "    second token, and so on.\n",
    "    \n",
    "    Note: in example in next cell, batch size is 2\n",
    "    \n",
    "    :param data: data as 1D tensor\n",
    "    :param batch_size: size of a single batch\n",
    "    :return: a 2D tensor in which each row is a batch\n",
    "    \"\"\"\n",
    "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // batch_size\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * batch_size)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(batch_size, -1).t().contiguous()\n",
    "    if cuda.is_available():\n",
    "        data = data.cuda()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 batches: the words\n",
      "      Once          a\n",
      "      upon       good\n",
      "         a       king\n",
      "      time        and\n",
      "     there          a\n",
      "       was      queen\n",
      "\n",
      "6 batches: the indexes\n",
      "      9917         46\n",
      "       845       1171\n",
      "        46       2463\n",
      "        23         90\n",
      "       994         46\n",
      "      1538       5574\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_data = \"Once upon a time there was a good king and a queen\"\n",
    "dummy_data_idx = [corpus.dictionary.word2idx[w] for w in dummy_data.split()]\n",
    "dummy_tensor = torch.LongTensor(dummy_data_idx) \n",
    "op = batchify(dummy_tensor, 2)\n",
    "print(f\"{len(op)} batches: the words\")\n",
    "for row in op:\n",
    "    print(\"%10s %10s\" %  (corpus.dictionary.idx2word[row[0]], corpus.dictionary.idx2word[row[1]]))\n",
    "print(f\"\\n{len(op)} batches: the indexes\")\n",
    "for row in op:\n",
    "    print(\"%10d %10d\" %  (row[0], row[1]))\n",
    "op.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_train = 20       # batch size for training set\n",
    "bs_valid = 10       # batch size for validation set\n",
    "# CRASHISSUE\n",
    "#bptt_size = 35      # number of times to unroll the graph for back propagation through time\n",
    "bptt_size = 25      # number of times to unroll the graph for back propagation through time\n",
    "clip = 0.25         # gradient clipping to check exploding gradient\n",
    "\n",
    "embed_size = 200    # size of the embedding vector\n",
    "hidden_size = 200   # size of the hidden state in the RNN \n",
    "num_layers = 2      # number of RNN layres to use\n",
    "dropout_pct = 0.5   # %age of neurons to drop out for regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = batchify(corpus.train, bs_train)\n",
    "val_data = batchify(corpus.valid, bs_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([51995, 20])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ~52,000 batches (rows), each of size 20 (columns)\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiments added by Ryan**\n",
    "*(feel free to delete or whatever)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dumb = torch.tensor([[0, 1], [2, 3], [4, 5]])\n",
    "dumb.shape\n",
    "dumb_flattened = dumb.view(-1)\n",
    "dumb_flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_size: 74010, embed_size: 200, hidden_size: 200, num_layers: 2\n",
    "model = RNNModel(vocab_size, embed_size, hidden_size, num_layers, dropout_pct)\n",
    "\n",
    "if cuda.is_available():\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_series(source, i, evaluation=False):\n",
    "    \"\"\"\n",
    "    Given a tensor containing batch data, extract a sequence starting at index i (i.e. batch i). \n",
    "    Sequence length is bptt_size or less.\n",
    "    \n",
    "    Again, each row of returned data is a batch, each column of row belongs to a particular\n",
    "    sequence of words. Technically, this returns a table of batch data.\n",
    "    \n",
    "    The returned 'target' is a little confusing. Basically, it just returns a 1-D tensor containing\n",
    "    the contents of the second row in returned `data`, followed by the contents of the third row,\n",
    "    etc.\n",
    "    \n",
    "    Obviously, the goal of this whole thing is to discover a relationship between words in one\n",
    "    batch and words in the next batch.\n",
    "    \n",
    "    :returns: (extracted sequence as tensor, a same-length sequence but one time step forward\n",
    "        and flattened)\n",
    "    \"\"\"\n",
    "    # Sequence length is whatever is smaller: number of unrollings or remaining entries in batch,\n",
    "    # past index i\n",
    "    seq_len = min(bptt_size, len(source) - 1 - i)\n",
    "    data = Variable(source[i:i+seq_len])\n",
    "    target = Variable(source[i+1:i+1+seq_len].view(-1))\n",
    "    if cuda.is_available():\n",
    "        data = data.cuda()\n",
    "        target = target.cuda()\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, target = get_batch_series(train_data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.shape = 25 x 20\n",
      "target.shape = torch.Size([500])\n"
     ]
    }
   ],
   "source": [
    "# Num batches x batch size\n",
    "print(f\"data.shape = {data.shape[0]} x {data.shape[1]}\")\n",
    "print(\"target.shape =\", target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([500])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More experiments by Ryan**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range:  range(0, 10)\n",
      "batch series: 0, i: 0\n",
      "batch series 0/index 0, targets 'THE', 'discontents', 'will', 'imprison'd.', '<eos>', 'like', 'the', 'be', 'USHER', 'am'\n",
      "batch series 0/index 0, seq 0; words are: <eos> THE SONNETS <eos> <eos> 1 <eos> <eos> From fairest\n",
      "batch series 0/index 0, seq 1; words are: The discontents repair, and men's reports <eos> Give him much\n",
      "batch series 0/index 0, seq 2; words are: beg will not become me. My <eos> way is to\n",
      "batch series 0/index 0, seq 3; words are: she imprison'd. All <eos> Is outward sorrow, though I think\n",
      "batch series 0/index 0, seq 4; words are: <eos> <eos> PLAYER QUEEN. <eos> So many journeys may the\n",
      "batch series: 1, i: 25\n",
      "batch series 1/index 25, targets 'riper', 'the', 'love', 'FIRST', 'is', 'than', 'is', '<eos>', 'the', 'them'\n",
      "batch series 1/index 25, seq 0; words are: the riper should by time decease, <eos> His tender heir\n",
      "batch series 1/index 25, seq 1; words are: from the primal state <eos> That he which is was\n",
      "batch series 1/index 25, seq 2; words are: the love you bear to men, to like as much\n",
      "batch series 1/index 25, seq 3; words are: <eos> FIRST GENTLEMAN. He that hath lost her too. So\n",
      "batch series 1/index 25, seq 4; words are: woe is me, you are so sick of late, <eos>\n",
      "batch series: 2, i: 50\n",
      "batch series: 3, i: 75\n",
      "batch series: 4, i: 100\n",
      "batch series: 5, i: 125\n",
      "batch series: 6, i: 150\n",
      "batch series: 7, i: 175\n",
      "batch series: 8, i: 200\n",
      "batch series: 9, i: 225\n",
      "batch series: 10, i: 250\n"
     ]
    }
   ],
   "source": [
    "print(\"range: \", range(0, 10))\n",
    "for batch_series_num, i in enumerate(range(0, train_data.size(0) - 1, bptt_size)):\n",
    "    print(f\"batch series: {batch_series_num}, i: {i}\")\n",
    "    if batch_series_num == 0 or batch_series_num == 1:\n",
    "        data, targets = get_batch_series(train_data, i)\n",
    "        targets_str = \"\"\n",
    "        for t in range(10):\n",
    "            t_word = corpus.dictionary.idx2word[targets[t]]\n",
    "            targets_str += f\"{', ' if t > 0 else ''}'{t_word}'\"\n",
    "        print(f\"batch series {batch_series_num}/index {i}, targets {targets_str}\")\n",
    "        # First 5 columns\n",
    "        for seq in range(5):\n",
    "            # First 10 rows\n",
    "            words = [corpus.dictionary.idx2word[data[w][seq]] for w in range(10)]\n",
    "            print(f\"batch series {batch_series_num}/index {i}, seq {seq}; words are: {' '.join(words)}\")\n",
    "    if batch_series_num >= 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_source, lr):\n",
    "    \"\"\"\n",
    "    :param data_source: the training data, split into 20 batches, as described in batchify() docstring\n",
    "    :param lr: loss rate\n",
    "    \"\"\"\n",
    "    # Turn on training mode which enables dropout.\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    hidden = model.init_hidden(bs_train)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    for batch_series_num, i in enumerate(range(0, data_source.size(0) - 1, bptt_size)):\n",
    "        \n",
    "        data, targets = get_batch_series(data_source, i)\n",
    "\n",
    "        # Original comment below -- I think he's using \"batch\" to mean \"batch series\"\n",
    "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "        hidden = Variable(hidden.data)\n",
    "        \n",
    "        if cuda.is_available():\n",
    "            hidden = hidden.cuda()\n",
    "        \n",
    "        # model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output, hidden = model(data, hidden)\n",
    "        loss = criterion(output.view(-1, vocab_size), targets)\n",
    "        loss.backward()\n",
    "\n",
    "        # `clip_grad_norm_` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += len(data) * loss.data\n",
    "        \n",
    "    return total_loss.item() / len(data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(data_source):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    hidden = model.init_hidden(bs_valid)\n",
    "    \n",
    "    for i in range(0, data_source.size(0) - 1, bptt_size):\n",
    "        data, targets = get_batch_series(data_source, i, evaluation=True)\n",
    "        \n",
    "        if cuda.is_available():\n",
    "            hidden = hidden.cuda()\n",
    "            \n",
    "        output, hidden = model(data, hidden)\n",
    "        output_flat = output.view(-1, vocab_size)\n",
    "        \n",
    "        total_loss += len(data) * criterion(output_flat, targets).data\n",
    "        hidden = Variable(hidden.data)\n",
    "        \n",
    "    return total_loss.item() / len(data_source)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over epochs.\n",
    "best_val_loss = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def run(epochs, lr):\n",
    "    global best_val_loss\n",
    "    \n",
    "    for epoch in range(0, epochs):\n",
    "        print(\"Beginning training for epoch:\", epoch)\n",
    "        start_time = datetime.now()\n",
    "        train_loss = train(train_data, lr)\n",
    "        val_loss = evaluate(val_data)\n",
    "        end_time = datetime.now()\n",
    "        time_delta = int((end_time - start_time).total_seconds())\n",
    "        minutes = time_delta / 60\n",
    "        seconds = time_delta % 60\n",
    "        print(f\"Finished epoch {epoch} in {minutes}:{seconds : 03d}\")\n",
    "        print(\"Train Loss: \", train_loss, \"Valid Loss: \", val_loss)\n",
    "\n",
    "        if not best_val_loss or val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), \"./4.model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning training for epoch: 0\n",
      "first forward() call, input shape is torch.Size([25, 20])\n",
      "hidden shape is torch.Size([2, 20, 200])\n",
      "emb shape is torch.Size([25, 20, 200])\n",
      "output shape is torch.Size([25, 20, 200])\n",
      "Finished epoch 0 in 2.4166666666666665: 25\n",
      "Train Loss:  6.846469011443408 Valid Loss:  6.875562963181961\n",
      "Beginning training for epoch: 1\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "run(5, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss:  5.9627584383113765 Valid Loss:  6.722295928926206\n",
      "Train Loss:  5.916144581209732 Valid Loss:  6.725216069457584\n",
      "Train Loss:  5.878729925954419 Valid Loss:  6.75531858542258\n",
      "Train Loss:  5.848820199057601 Valid Loss:  6.768370831362346\n",
      "Train Loss:  5.825219372055005 Valid Loss:  6.7805644660596025\n"
     ]
    }
   ],
   "source": [
    "run(5, 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 200\n",
    "temperature = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNModel(\n",
       "  (encoder): Embedding(74010, 200)\n",
       "  (drop1): Dropout(p=0.5)\n",
       "  (drop2): Dropout(p=0.5)\n",
       "  (rnn): GRU(200, 200, num_layers=2, dropout=0.5)\n",
       "  (decoder): Linear(in_features=200, out_features=74010, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNNModel(vocab_size, embed_size, hidden_size, num_layers, dropout_pct)\n",
    "model.load_state_dict(torch.load(\"./4.model.pth\"))\n",
    "\n",
    "if cuda.is_available():\n",
    "    model.cuda()\n",
    "    \n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://nlp.stanford.edu/blog/maximum-likelihood-decoding-with-rnns-the-good-the-bad-and-the-ugly/\n",
    "# Which sample is better? It depends on your personal taste. The high temperature \n",
    "# sample displays greater linguistic variety, but the low temperature sample is \n",
    "# more grammatically correct. Such is the world of temperature sampling - lowering \n",
    "# the temperature allows you to focus on higher probability output sequences and \n",
    "# smooth over deficiencies of the model.\n",
    "\n",
    "# If we set a high temperature, we can get more entropic (*noisier*) probabilities\n",
    "# Often we want to sample with low temperatures to produce sharp probabilities\n",
    "temperature = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a cog, \n",
      "A trumpet with his sham'd. \n",
      "SECOND MURDERER. I have say make you be of your now. \n",
      "PETER. He shall not be afeard against my life, for her, to the \n",
      "music of my name to a thence is the Queen of the \n",
      "tanner of my own posterity, \n",
      "WOLSEY. I know the matter, Is a Boult, an steward \n",
      "I'll hear the mercy when they are you? \n",
      "The valour is the Empress' innocent \n",
      "At heaven unto the flesh, have a Antony, \n",
      "Yet, like the man of high all men will am \n",
      "And nothing to faith, the happiness of his war. \n",
      "FIRST MURDERER. What goes the thousand king of his man's minds in \n",
      "reason. \n",
      "\n",
      "Come on him here. \n",
      "\n",
      "Enter PISANIO and drum and LORD \n",
      "\n",
      "CAPULET. \n",
      "My poor lord, I are a letter in \n",
      "To see them. Let the King be absent. \n",
      "I am that woman, can not into a weapon \n",
      "To us a thousand more. \n",
      "\n",
      "Set of the master? \n",
      "\n",
      "HAMLET. \n",
      "How bright my council lie up to the exceeds and cross \n",
      "bade "
     ]
    }
   ],
   "source": [
    "hidden = model.init_hidden(1)\n",
    "idx = corpus.dictionary.word2idx['I']\n",
    "input = Variable(torch.LongTensor([[idx]]).long(), volatile=True)\n",
    "\n",
    "if cuda.is_available():\n",
    "    input.data = input.data.cuda()\n",
    "\n",
    "print(corpus.dictionary.idx2word[idx], '', end='')\n",
    "\n",
    "for i in range(num_words):\n",
    "    output, hidden = model(input, hidden)\n",
    "    word_weights = output.squeeze().data.div(temperature).exp().cpu()\n",
    "    word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "    input.data.fill_(word_idx)\n",
    "    word = corpus.dictionary.idx2word[word_idx]\n",
    "\n",
    "    if word == '<eos>':\n",
    "        print('')\n",
    "    else:\n",
    "        print(word + ' ', end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Homework**\n",
    "\n",
    "* Play with the hyperparameters\n",
    "* Play with the model architecture\n",
    "* Run this on a different dataset\n",
    "* Search up: Perplexity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
